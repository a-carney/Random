Alex Carney
COS 470 - Text Analytics using AI
Course Project - Part 1

ROUGH DRAFT
	In an era where digital interactions have become more prevalent than ever, the ability to control the spread of information has continued to be a major concern. One of the more important issues is the rise in online toxic language, specifically in social media forums. This toxic language, often referred to colloquially as things like "hate speech" or "cyberbully" can have significant impacts on the mental health of those who are targeted. As a result, there has been a signifigant effort to develop tools that can detect and prevent the spread of toxic information. Though many efforts are ongoing to block hate speech, less effort has been put into the development of tools that can cleanse the language of its toxic elements while preserving original context. This process, known as text detoxification, is discussed in this paper along with strategies that can be implemented. Although it is clear that many different strategies need to work together for this to completely work succesfully, this paper will also explain why the specific use of explainable artifical intelligence (XAI) and ensemble-based marchine learning models are critical to this accomplishment.

	In the last decade, our world has seen technology connect people more than ever before. Social media platforms like Facebook, Twitter, and Instagram have allowed users to connect with people in a fast, efficient, but often impersonal way. This has led to a rise in the amount of toxic language that is spread online, and many computer scientists have been working to develop tools that can help mitigate this. Recently, the use of machine learning models that specialize in natural language processing (NLP) have been used to detect and block toxic language. These models are typically trained on large datasets of text that have been labeled into different categeories.  With the relatively recent launch of BERT in 2018 from Google, we have seen a significant improvement in the ability of these models to understand the context of phrases, which has also helped greatly with the advancement of these technologies. 

	However, these models are not perfect, and most reserach has shown that a particularly difficult issue is toxic language detectors returning false postivies (FP), or in other words, detecting toxic language where there is none. Although the converse, a false negative (FN), is still something that occurs, false postiives are particularly harmful in online social environments where a user could be wrongly removed from a network. Due to these additional cases, this paper will always highglight the importance of approaching text dextoficiation in a nuanced way by leveraging explainable artificial intelligence (XAI) and ensemble-based machine learning models.

----------------------------------------------------------------------------------------------------------------------------

	With many languages models, includeing BERT, becoming availalbe durings the 2010s, the ability to understand the context of phrases has greatly improvd. Before, most language processing was developed with the use of simpler tools like row filtering, tokenization, stop word removal, and regex. These tools were not able to understand the context of phrases, and as a result, were not great at language processing. However, with the advent of more complex models, we began to see models underestand more context. 

	Almost immediately, it was clear that these models experienced signifigant issues with False Positives (FP). In 2020, a study at Cornell University sought to find the issues with creating a proper hate speech detection model. The study noted that while there was a clear demand for this, the models would often return false positives, and disproportionately assigned FP to miniority groups. In fact, it was pointed out that African Americans were the mostl likely to get flagged for a false posiotive. While a robust solution is not provided, the study does show that the introduction of additional information is nessary to help classify samples with more information that the language itself. Things like the user's ethnicity could be used to help reduce the number of FPs. [8]		 

	Another study at the University of Oulu stated that the rise of toxic speech can largely be attributed to the comfort of anonymity that the internet provides, and the slowness of human moderation. [2]
	-------------------------------------------------------------------------------------------------
-----------
	
